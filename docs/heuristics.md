---
title: Heuristics
layout: default
---

![](https://divergentmba.files.wordpress.com/2010/04/knowledge-funnel-762867.png){: align="right" width="350px"}

- Your words are most useful when they let me cut my code
   - yes, this is all "just reinforcement learning" and "just bayesian parameter
     optimization" and "just search-based SE" and "just optimzation"
   - but the devil is in the detail. if we lean in to how to write wokrable systems for
     all the above then 

- Beliefs are sticky things
   - hang around much longer than they should
   - need to always be revising

- The technology that will most improve data science is.. science
   - Science = a cache of concepts curated by a community, doing each
     other the courtesy of (re)checking and improving each other's ideas.
   - Most data science is not science

- Not general models, but general methods for local models.

- Tom (Zimmermann)'s first law: data has answers, for the questions you aren't asking (yet)

- Tom's second law:
   - projects mature from "many one-off" queries to a "just a few repeated"  ones

- Tim's first law: ask a lot of questions, expect a few answers

- Wei's Law: It will take 3+ months.

- Wei's second law: One data scientists per two (or three) data engineers

- David (Wolpert)'s law: no such thing as a free lunch (no learner or optimzier is always best).
   - So your going to have to look around some

- George's law: better learners/optimizer are rare, so better better is even rarer rarer.
   - So you don't have to look around forever

- Fisher's Law: "Fast iteration is key"

- Timm's law: AI should be agile and often it aint

- Dave (Binkley)'s rule: Your learners ain't my learners
   - Software specific inference

- Dieter's Law: Less, But Better
   - Particularly when exploring options

- Most data is crap data PCA, 1901
Narrows: Amarel 1960s
Prototypes: Chen 1975
Frames: Minsky, 1975
Min environments: DeKleer, 1986
Saturation: Horgan & Mathur: 1980
Homogeneous propagation: Michael: 1981
Master variables: Crawford & Baker, 1995
Clumps, Druzdel, 1997
Feature subset section, Kohavi, 1997,
Back doors, Williams, 2002
Active learning: many people (2000+)


- Lofti's Law:
  As complexity rises, precise statements lose meaning and meaningful statements lose precision.

- Ken's law: I want to read symbols. 
   - Qualitative representations—symbolic representations that carve continuous phenomena into meaningful units—are central to human cognition.
   -  They provide a foundation for expert reasoning in science and engineering by making explicit the broad categories of things that might happen and enabling causal models that help guide the application of more quantitative knowledge as needed. 

- Zach's law: start as you mean to go on

- Vivek's law:  guess is faster than knowing (surrogates)

- Jack's law: a couple of darts beats smarts


- Amrit's law : most differences, aren't

- Tim's first law: not studying theory is bad

- Tim's second law: too little theory is a very bad.

- Tim's third law: too much theory is a very very bad.

- Tim's fourth law: step away from the trivial
   - not tiny effects, bit ones
   - coarse grained states

- Tim's fifth law: pictures for intuitions, stats for sanity

- Tim's sixth law: localize!

- Data miners for one goal, optimizers for N

- Suvodeep's rule: If your optimizer is slow, add a data miner

- Martin's rule: if your optimizer is confusing, add a data miner.

- Jianfeng's first law: verfication is faster than repair, repair is faster than generate

- Jianfeng's second law: the deltas between valid examples are (usually) valid

- Jianfeng's third law: things that need repair are more interesting than otherwise



----

## Design Choices for Systems

- Speed of model
- Speed of constraints
